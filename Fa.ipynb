{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Mask RCNN FashionAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/logs/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"logs/mask_rcnn_coco.h5\")\n",
    "print(COCO_MODEL_PATH)\n",
    "    \n",
    "'''添加fashion ai'''\n",
    "fi_class_names_=['neckline_left', 'neckline_right', 'center_front', 'shoulder_left', \n",
    "                'shoulder_right', 'armpit_left','armpit_right', 'waistline_left', \n",
    "                'waistline_right', 'cuff_left_in','cuff_left_out', 'cuff_right_in', \n",
    "                'cuff_right_out', 'top_hem_left','top_hem_right', 'waistband_left', \n",
    "                'waistband_right', 'hemline_left','hemline_right', 'crotch',\n",
    "                'bottom_left_in', 'bottom_left_out','bottom_right_in', 'bottom_right_out']\n",
    "fi_class_names=['clothing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FIConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"FI\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Uncomment to train on 8 GPUs (default is 1)\n",
    "    # GPU_COUNT = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    # NUM_CLASSES = 1 + 80  # COCO has 80 classes\n",
    "    NUM_CLASSES = 1 + 1  # Person and background\n",
    "\n",
    "    NUM_KEYPOINTS = 24\n",
    "    MASK_SHAPE = [28, 28]\n",
    "    KEYPOINT_MASK_SHAPE = [56,56]\n",
    "    # DETECTION_MAX_INSTANCES = 50\n",
    "    TRAIN_ROIS_PER_IMAGE = 100\n",
    "    MAX_GT_INSTANCES = 128\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 150\n",
    "    USE_MINI_MASK = True\n",
    "    MASK_POOL_SIZE = 14\n",
    "    KEYPOINT_MASK_POOL_SIZE = 7\n",
    "    LEARNING_RATE = 0.002\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "    WEIGHT_LOSS = True\n",
    "    KEYPOINT_THRESHOLD = 0.005\n",
    "config = FIConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FIDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "    \"\"\"参数:category决定数据类别为train validation test\"\"\"\n",
    "        \n",
    "    def load_FI(self,category):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        train_data_path='./data/train/'\n",
    "        # Add classes\n",
    "        for i,class_name in enumerate(fi_class_names):\n",
    "            self.add_class(\"FI\", i+1,class_name)\n",
    "            \n",
    "        annotations=pd.read_csv('./data/train/Annotations/annotations.csv')\n",
    "        annotations=annotations.append(pd.read_csv('./data/train/Annotations/train.csv'),ignore_index=True)\n",
    "        \n",
    "        #切分test数据集和train数据集\n",
    "        np.random.seed(42)\n",
    "        shuffled_indces=np.random.permutation(annotations.shape[0])\n",
    "        \n",
    "        val_set_size=int(annotations.shape[0]*0.01)\n",
    "        val_indices=shuffled_indces[:val_set_size]\n",
    "        train_indices=shuffled_indces[val_set_size:]\n",
    "        \n",
    "        \n",
    "        if category =='train':\n",
    "            annotations=annotations.iloc[train_indices]\n",
    "        elif category=='val':\n",
    "            annotations=annotations.iloc[val_indices]\n",
    "        else:\n",
    "            #test 数据集\n",
    "            pass\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        \n",
    "        annotations=annotations.reset_index(drop=True)#更新索引\n",
    "        \n",
    "        for x in range(annotations.shape[0]):\n",
    "            #bg_color, shapes = self.random_image(height, width)\n",
    "            id=annotations.loc[x,'image_id']\n",
    "            category=annotations.loc[x,'image_category']\n",
    "            \n",
    "            im_path=os.path.join(train_data_path,id)\n",
    "    \n",
    "            height,width=mpimg.imread(im_path).shape[0:2]\n",
    "        \n",
    "            key_points=[]\n",
    "            for key_point in annotations.loc[x,fi_class_names_].values:\n",
    "                loc_cat=[int(j) for j in key_point.split('_')]\n",
    "                key_points.append(loc_cat)\n",
    "            \n",
    "            self.add_image(\"FI\", image_id=id, path=im_path,\n",
    "                           width=width-1, height=height-1,\n",
    "                           key_points=key_points,image_category=category)#添加我的数据\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        根据image_id读取图片\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        image=mpimg.imread(info['path'])\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the key_points data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"FI\":\n",
    "            return info[\"key_points\"],info[\"image_category\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "    def load_keypoints(self, image_id):\n",
    "        \"\"\"Load clothing keypoints for the given image.\n",
    "\n",
    "        Returns:\n",
    "        key_points: num_keypoints coordinates and visibility (x,y,v)  [num_person,num_keypoints,3] of num_person\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks, here is always equal to [num_person, 1]\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"FI\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        keypoints = []\n",
    "        class_ids = []\n",
    "        instance_masks = []\n",
    "        annotations = self.image_info[image_id][\"key_points\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['key_points']))\n",
    "            assert class_id == 1\n",
    "            if class_id:\n",
    "\n",
    "                #load masks\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                #load keypoints\n",
    "                keypoint = annotation[\"keypoints\"]\n",
    "                keypoint = np.reshape(keypoint,(-1,3))\n",
    "\n",
    "                keypoints.append(keypoint)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            keypoints = np.array(keypoints,dtype=np.int32)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            masks = np.stack(instance_masks, axis=2)\n",
    "            return keypoints, masks, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_keypoints(image_id)\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        key_points=np.array(info['key_points'])\n",
    "        clothing_nums=int(len(key_points)/24)\n",
    "        \n",
    "        m = np.zeros([clothing_nums,info['height'], info['width'], 24])#生成24个mask,因为有24个关键点。\n",
    "        \n",
    "        class_mask = np.zeros([clothing_nums,24])  #点存在的状态经过处理有三种状态 不存在为0  1为不可见.2 为可见 三分类\n",
    "        class_ids=[]\n",
    "       \n",
    "        for clothing_num in range(clothing_nums):\n",
    "            \n",
    "            for part_num, bp in enumerate(key_points):\n",
    "                if bp[2] > -1: #AI数据编码为bp[2]=1为可见，=2为不可见，=3为不在图内或者不可推测，FI编码为=-1为不存在，0为不可见，1为可   \n",
    "                    m[clothing_num,bp[1],bp[0],part_num] = 1\n",
    "                    class_mask[clothing_num,part_num] = bp[2]+1\n",
    "            class_ids.append(1)\n",
    "            \n",
    "        #Pach instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = m\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids, class_mask   \n",
    "        else:\n",
    "            return super(self.__class__).load_mask(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['BG', 'clothing'].\n\nTrain Images: 43724.\n\nValid Images: 441\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "dataset_train = FIDataset()\n",
    "dataset_train.load_FI(category='train')\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = FIDataset()\n",
    "dataset_val.load_FI(category='val')\n",
    "dataset_val.prepare()\n",
    "\n",
    "print(\"Classes: {}.\\n\".format(dataset_train.class_names))\n",
    "print(\"Train Images: {}.\\n\".format(len(dataset_train.image_ids)))\n",
    "print(\"Valid Images: {}\".format(len(dataset_val.image_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/logs/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"training\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True,exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "print(\"Loading weights from \", COCO_MODEL_PATH)\n",
    "# model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train heads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStarting at epoch 0. LR=0.002\n\nCheckpoint Path: /home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/logs/fi20180330T1331/mask_rcnn_fi_{epoch:04d}.h5\nSelecting layers to train\nfpn_c5p5               (Conv2D)\nfpn_c4p4               (Conv2D)\nfpn_c3p3               (Conv2D)\nfpn_c2p2               (Conv2D)\nfpn_p5                 (Conv2D)\nfpn_p2                 (Conv2D)\nfpn_p3                 (Conv2D)\nfpn_p4                 (Conv2D)\nIn model:  rpn_model\n    rpn_conv_shared        (Conv2D)\n    rpn_class_raw          (Conv2D)\n    rpn_bbox_pred          (Conv2D)\nmrcnn_keypoint_mask_conv1   (TimeDistributed)\nmrcnn_keypoint_mask_bn1   (TimeDistributed)\nmrcnn_keypoint_mask_conv2   (TimeDistributed)\nmrcnn_keypoint_mask_bn2   (TimeDistributed)\nmrcnn_keypoint_mask_conv3   (TimeDistributed)\nmrcnn_keypoint_mask_bn3   (TimeDistributed)\nmrcnn_keypoint_mask_conv4   (TimeDistributed)\nmrcnn_keypoint_mask_bn4   (TimeDistributed)\nmrcnn_keypoint_mask_conv5   (TimeDistributed)\nmrcnn_keypoint_mask_bn5   (TimeDistributed)\nmrcnn_keypoint_mask_conv6   (TimeDistributed)\nmrcnn_mask_conv1       (TimeDistributed)\nmrcnn_keypoint_mask_bn6   (TimeDistributed)\nmrcnn_mask_bn1         (TimeDistributed)\nmrcnn_keypoint_mask_conv7   (TimeDistributed)\nmrcnn_mask_conv2       (TimeDistributed)\nmrcnn_keypoint_mask_bn7   (TimeDistributed)\nmrcnn_mask_bn2         (TimeDistributed)\nmrcnn_class_conv1      (TimeDistributed)\nmrcnn_class_bn1        (TimeDistributed)\nmrcnn_keypoint_mask_conv8   (TimeDistributed)\nmrcnn_mask_conv3       (TimeDistributed)\nmrcnn_keypoint_mask_bn8   (TimeDistributed)\nmrcnn_mask_bn3         (TimeDistributed)\nmrcnn_class_conv2      (TimeDistributed)\nmrcnn_class_bn2        (TimeDistributed)\nmrcnn_keypoint_mask_deconv   (TimeDistributed)\nmrcnn_mask_conv4       (TimeDistributed)\nmrcnn_mask_bn4         (TimeDistributed)\nmrcnn_bbox_fc          (TimeDistributed)\nmrcnn_mask_deconv      (TimeDistributed)\nmrcnn_class_logits     (TimeDistributed)\nmrcnn_mask             (TimeDistributed)\nWARNING:tensorflow:From /home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py:2884: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/skirt/034d23b57ff1f8020b186854d627d597.jpg', 'image_category': 'skirt', 'key_points': [[-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [199, 218, 1], [279, 234, 1], [155, 417, 1], [292, 446, 1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1]], 'source': 'FI', 'id': 'Images/skirt/034d23b57ff1f8020b186854d627d597.jpg', 'width': 479}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/blouse/e9231dc8192c8bb73d21f17f0aa7002b.jpg', 'image_category': 'blouse', 'key_points': [[241, 71, 1], [294, 67, 1], [266, 80, 1], [163, 117, 1], [378, 122, 1], [164, 224, 1], [362, 232, 1], [-1, -1, -1], [-1, -1, -1], [150, 433, 1], [126, 433, 1], [380, 440, 1], [405, 441, 1], [187, 446, 1], [340, 451, 1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1]], 'source': 'FI', 'id': 'Images/blouse/e9231dc8192c8bb73d21f17f0aa7002b.jpg', 'width': 511}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/trousers/20ef1d53a9ea239ed1c8b89a3bcdf0dc.jpg', 'image_category': 'trousers', 'key_points': [[-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [150, 179, 1], [213, 168, 1], [-1, -1, -1], [-1, -1, -1], [189, 232, 1], [219, 378, 1], [192, 388, 1], [196, 381, 0], [221, 379, 1]], 'source': 'FI', 'id': 'Images/trousers/20ef1d53a9ea239ed1c8b89a3bcdf0dc.jpg', 'width': 383}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/skirt/d95ba38d4615f31d88ef19930e93c93f.jpg', 'image_category': 'skirt', 'key_points': [[-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [222, 256, 1], [318, 244, 1], [221, 443, 1], [373, 431, 1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1]], 'source': 'FI', 'id': 'Images/skirt/d95ba38d4615f31d88ef19930e93c93f.jpg', 'width': 511}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/blouse/5ed30456a56df2d8ee0f62d45c748a8d.jpg', 'image_category': 'blouse', 'key_points': [[245, 131, 1], [299, 139, 0], [276, 157, 1], [189, 159, 1], [352, 176, 1], [208, 244, 1], [331, 249, 0], [-1, -1, -1], [-1, -1, -1], [202, 294, 1], [144, 279, 1], [326, 304, 1], [367, 303, 1], [195, 366, 1], [340, 367, 1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1]], 'source': 'FI', 'id': 'Images/blouse/5ed30456a56df2d8ee0f62d45c748a8d.jpg', 'width': 511}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'height': 511, 'path': './data/train/Images/skirt/87ada8d86094e619123c259a9622d5a0.jpg', 'image_category': 'skirt', 'key_points': [[-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [118, 55, 1], [231, 54, 1], [54, 360, 1], [290, 365, 1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1], [-1, -1, -1]], 'source': 'FI', 'id': 'Images/skirt/87ada8d86094e619123c259a9622d5a0.jpg', 'width': 340}\nTraceback (most recent call last):\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 2195, in data_generator_keypoint\n    #image_meta:image_id,image_shape,windows.active_class_ids\n  File \"/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\", line 1735, in load_image_gt_keypoints\n    # mask, class_ids = dataset.load_mask(image_id)\n  File \"<ipython-input-10-d0f5cee65612>\", line 102, in load_keypoints\n    \"coco.{}\".format(annotation['key_points']))\nTypeError: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a056e22a5140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             layers='heads')\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Training - Stage 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Finetune layers from ResNet stage 4 and up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers)\u001b[0m\n\u001b[1;32m   3044\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3046\u001b[0;31m         self.keras_model.fit_generator(\n\u001b[0m\u001b[1;32m   3047\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\u001b[0m in \u001b[0;36mdata_generator_keypoint\u001b[0;34m(dataset, config, shuffle, augment, random_rois, batch_size, detection_targets)\u001b[0m\n\u001b[1;32m   2193\u001b[0m             \u001b[0;31m# Get GT bounding boxes and masks for image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m             \u001b[0;31m#image_meta:image_id,image_shape,windows.active_class_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_keypoints\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                 \u001b[0mload_image_gt_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mini_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSE_MINI_MASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shenhuaifeng/Documents/Mask_RCNN_Humanpose-master/model.py\u001b[0m in \u001b[0;36mload_image_gt_keypoints\u001b[0;34m(dataset, config, image_id, augment, use_mini_mask)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Load image and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     \u001b[0;31m# mask, class_ids = dataset.load_mask(image_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d0f5cee65612>\u001b[0m in \u001b[0;36mload_keypoints\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             class_id = self.map_source_class_id(\n\u001b[0;32m--> 102\u001b[0;31m                 \"coco.{}\".format(annotation['key_points']))\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Training - Stage 1\n",
    "print(\"Train heads\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=15,\n",
    "            layers='heads')\n",
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Training Resnet layer 4+\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=20,\n",
    "            layers='4+')\n",
    "# Training - Stage 3\n",
    "# Finetune layers from ResNet stage 3 and up\n",
    "print(\"Training Resnet layer 3+\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 100,\n",
    "            epochs=100,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}