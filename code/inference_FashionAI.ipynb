{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN for FashionAi key point location\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "如何使用本代码：\n",
    "只需修改：Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"dress_logs\")\n",
    "fi_class_name中的类别，\n",
    "和FIconfig类中的NUM_KEYPOINTS\n",
    "分别训练5个单独的类\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "#import visualize\n",
    "from model import log\n",
    "from PIL import Image\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#设置gpu内存动态增长\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs/blouse_logs\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '../'\n",
    "\n",
    "fi_class_names = ['blouse']\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs/{}_logs\".format(fi_class_names[0]))\n",
    "SELF_MODEL_PATH=os.path.join(ROOT_DIR,\"model/mask_rcnn_{}.h5\".format(fi_class_names[0]))\n",
    "print(MODEL_DIR)\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"model/mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neckline_left', 'neckline_right', 'center_front', 'shoulder_left', 'shoulder_right', 'armpit_left', 'armpit_right', 'cuff_left_in', 'cuff_left_out', 'cuff_right_in', 'cuff_right_out', 'top_hem_left', 'top_hem_right']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "添加fashion ai\n",
    "5类衣服的所有点\n",
    "'''\n",
    "\n",
    "class_names_ = ['neckline_left', 'neckline_right', 'center_front', 'shoulder_left',\n",
    "                   'shoulder_right', 'armpit_left', 'armpit_right', 'waistline_left',\n",
    "                   'waistline_right', 'cuff_left_in', 'cuff_left_out', 'cuff_right_in',\n",
    "                   'cuff_right_out', 'top_hem_left', 'top_hem_right', 'waistband_left',\n",
    "                   'waistband_right', 'hemline_left', 'hemline_right', 'crotch',\n",
    "                   'bottom_left_in', 'bottom_left_out', 'bottom_right_in', 'bottom_right_out']\n",
    "\n",
    "blouse_index=[0,1,2,3,4,5,6,9,10,11,12,13,14]#NUM_KEYPOINTS=13\n",
    "skirt_index=[15,16,17,18]#NUM_KEYPOINTS=4\n",
    "outwear_index=[0,1,3,4,5,6,7,8,9,10,11,12,13,14]#NUM_KEYPOINTS=14\n",
    "dress_index=[0,1,2,3,4,5,6,7,8,9,10,11,12,17,18]#NUM_KEYPOINTS=15\n",
    "trousers_index=[15,16,19,20,21,22,23]#NUM_KEYPOINTS=7\n",
    "all_index={'blouse':blouse_index,\n",
    "           'skirt':skirt_index,\n",
    "           'outwear':outwear_index,\n",
    "           'dress':dress_index,\n",
    "           'trousers':trousers_index}\n",
    "index = all_index[fi_class_names[0]]\n",
    "\n",
    "\n",
    "fi_class_names_=[]\n",
    "for i in index:\n",
    "    fi_class_names_.append(class_names_[i])\n",
    "print(fi_class_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FIConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    IMAGE_CATEGORY=fi_class_names[0]\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"FI\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_KEYPOINTS =len(all_index[fi_class_names[0]])\n",
    "    KEYPOINT_MASK_SHAPE = [56, 56]\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 24 key_point\n",
    "\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 150\n",
    "    VALIDATION_STPES = 100\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "    MINI_MASK_SHAPE = (56, 56)\n",
    "    KEYPOINT_MASK_POOL_SIZE = 7\n",
    "            # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "    MASK_SHAPE = [28, 28]\n",
    "    WEIGHT_LOSS = True\n",
    "    KEYPOINT_THRESHOLD = 0.005\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "\n",
    "\n",
    "config = FIConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pic_height_width(filepath):\n",
    "    fp = open(filepath, 'rb')\n",
    "    im = Image.open(fp)\n",
    "    fp.close()\n",
    "    x, y = im.size\n",
    "    if(im.mode =='RGB'):\n",
    "        return x,y\n",
    "    else:\n",
    "        return False,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FIDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "    \"\"\"参数:category决定数据类别为train validation test\"\"\"\n",
    "\n",
    "    def load_FI(self, category):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        train_data_path = '../data/train/'\n",
    "        # Add classes\n",
    "        for i, class_name in enumerate(fi_class_names):\n",
    "            self.add_class(\"FI\", i + 1, class_name)\n",
    "\n",
    "        annotations = pd.read_csv('../data/train/Annotations/annotations.csv')\n",
    "        annotations = annotations.append(pd.read_csv('../data/train/Annotations/train.csv'), ignore_index=True)\n",
    "        annotations = annotations.append(pd.read_csv('../data/train/Annotations/test_a.csv'), ignore_index=True)\n",
    "        annotations = annotations.append(pd.read_csv('../data/train/Annotations/test_b.csv'), ignore_index=True)\n",
    "        annotations = annotations.append(pd.read_csv('../data/train/Annotations/data_scaling.csv'), ignore_index=True)\n",
    "        annotations = annotations.append(pd.read_csv('../data/train/Annotations/data_flip_up_down.csv'), ignore_index=True)\n",
    "        annotations = annotations.loc[annotations['image_category'] == fi_class_names[0]]\n",
    "        annotations = annotations.reset_index(drop=True)  # 更新索引\n",
    "        # 切分test数据集和train数据集\n",
    "        np.random.seed(42)\n",
    "        shuffled_indces = np.random.permutation(annotations.shape[0])\n",
    "        val_set_size = int(annotations.shape[0] * 0.05)\n",
    "        val_indices = shuffled_indces[:val_set_size]\n",
    "        train_indices = shuffled_indces[val_set_size:]\n",
    "        if category == 'train':\n",
    "            annotations = annotations.iloc[train_indices]\n",
    "        elif category == 'val':\n",
    "            annotations = annotations.iloc[val_indices]\n",
    "        else:\n",
    "            # test 数据集\n",
    "            pass\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "\n",
    "        annotations = annotations.reset_index(drop=True)  # 更新索引\n",
    "\n",
    "        for x in range(annotations.shape[0]):\n",
    "            # bg_color, shapes = self.random_image(height, width)\n",
    "            id = annotations.loc[x, 'image_id']\n",
    "            category = annotations.loc[x, 'image_category']\n",
    "            print('loading image:%d/%d'%(x,annotations.shape[0]))\n",
    "            im_path = os.path.join(train_data_path, id)\n",
    "\n",
    "            # height, width = cv2.imread(im_path).shape[0:2]\n",
    "            width, height = pic_height_width(im_path)\n",
    "\n",
    "            key_points = []\n",
    "            for key_point in annotations.loc[x, fi_class_names_].values:\n",
    "                loc_cat = [int(j) for j in key_point.split('_')]\n",
    "                key_points.append(loc_cat)\n",
    "\n",
    "            self.add_image(\"FI\", image_id=id, path=im_path,\n",
    "                           width=width, height=height,\n",
    "                           key_points=key_points, image_category=category)  # 添加我的数据\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        根据image_id读取图片\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        # image = cv2.imread(info['path'])\n",
    "        image = Image.open(info['path'])\n",
    "        image = np.array(image)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the key_points data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"FI\":\n",
    "            return info[\"key_points\"], info[\"image_category\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        key_points = np.array(info['key_points'])\n",
    "        clothing_nums = int(len(key_points) /config.NUM_KEYPOINTS)\n",
    "\n",
    "        m = np.zeros([clothing_nums, info['height'], info['width'], config.NUM_KEYPOINTS])  # 生成24个mask,因为有24个关键点。\n",
    "\n",
    "        class_mask = np.zeros([clothing_nums, config.NUM_KEYPOINTS])  # 点存在的状态经过处理有三种状态 不存在为0  1为不可见.2 为可见 三分类\n",
    "        class_ids = []\n",
    "\n",
    "        for clothing_num in range(clothing_nums):\n",
    "\n",
    "            for part_num, bp in enumerate(key_points):\n",
    "                if bp[2] > -1:  # AI数据编码为bp[2]=1为可见，=2为不可见，=3为不在图内或者不可推测，FI编码为=-1为不存在，0为不可见，1为可\n",
    "                    m[clothing_num, bp[1], bp[0], part_num] = 1\n",
    "                    class_mask[clothing_num, part_num] = bp[2] + 1\n",
    "            class_ids.append(1)\n",
    "        #class_ids=np.array([self.class_names.index(s[0]) for s in fi_class_names_])\n",
    "\n",
    "        # Pach instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = m\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids, class_mask\n",
    "        else:\n",
    "            return super(self.__class__).load_mask(image_id)\n",
    "\n",
    "    def load_keypoints(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        image_category=info['image_category']\n",
    "        key_points = np.array(info['key_points'])\n",
    "        clothing_nums = int(len(key_points) / config.NUM_KEYPOINTS)\n",
    "        keypoints = []\n",
    "        keypoint = []\n",
    "        class_ids = []\n",
    "        for clothing_num in range(clothing_nums):\n",
    "\n",
    "            for part_num, bp in enumerate(key_points):\n",
    "                # if bp[2] > -1:  # AI数据编码为bp[2]=1为可见，=2为不可见，=3为不在图内或者不可推测，FI编码为=-1为不存在，0为不可见，1为可\n",
    "                #     m[clothing_num, bp[1], bp[0], part_num] = 1\n",
    "                #     class_mask[clothing_num, part_num] = bp[2] + 1\n",
    "                if(bp[2]==-1):\n",
    "                    keypoint += [0, 0, 0]\n",
    "                else:\n",
    "                    keypoint += [bp[0]-1,bp[1]-1,bp[2]+1]\n",
    "            keypoint = np.reshape(keypoint,(-1,3))\n",
    "            keypoints.append(keypoint)\n",
    "            class_ids.append(1)\n",
    "        #class_ids=np.array([self.class_names.index(image_category)])#如果多分类使用此处\n",
    "        if class_ids:\n",
    "            keypoints = np.array(keypoints, dtype=np.int32)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return keypoints, 0, class_ids\n",
    "        else:\n",
    "            return super(self.__class__).load_keypoints(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
